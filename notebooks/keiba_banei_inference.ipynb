{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ã°ã‚“ãˆã„ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒª - å½“æ—¥äºˆæ¸¬\n",
    "\n",
    "**æœ€çµ‚æ›´æ–°: 2026-01-26**\n",
    "\n",
    "---\n",
    "\n",
    "**ã°ã‚“ãˆã„ç«¶é¦¬ï¼ˆå¸¯åºƒï¼‰å°‚ç”¨ã®äºˆæ¸¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯**\n",
    "\n",
    "## ã°ã‚“ãˆã„ç«¶é¦¬ã®ç‰¹æ€§\n",
    "\n",
    "- **ã‚³ãƒ¼ã‚¹**: ç›´ç·š200mï¼ˆå¸¯åºƒç«¶é¦¬å ´ã®ã¿ï¼‰\n",
    "- **é¦¬ä½“é‡**: 800ã€œ1200kgï¼ˆé€šå¸¸ç«¶é¦¬ã®ç´„2å€ï¼‰\n",
    "- **ã‚½ãƒªé‡é‡**: 480ã€œ1000kgï¼ˆæ–¤é‡ã®ä»£ã‚ã‚Šï¼‰\n",
    "- **éšœå®³**: 2ã¤ã®å‚ã‚’è¶Šãˆã‚‹\n",
    "- **é¨æ‰‹å½±éŸ¿åº¦**: é¦¬3:é¨æ‰‹7ï¼ˆé€šå¸¸ç«¶é¦¬ã¯é¦¬7:é¨æ‰‹3ï¼‰\n",
    "- **é¦¬å ´çŠ¶æ…‹**: æ°´åˆ†é‡ï¼ˆ%ï¼‰ã§è¡¨ç¤º\n",
    "\n",
    "## ä½¿ç”¨æ–¹æ³•\n",
    "1. å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ğŸ”‘ã‹ã‚‰ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã«ä»¥ä¸‹ã‚’è¨­å®šï¼š\n",
    "   - `SUPABASE_KEY`\n",
    "2. ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³1ã€œ6ï¼‰\n",
    "3. ã‚»ã‚¯ã‚·ãƒ§ãƒ³7ã§ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’æŒ‡å®šã—ã¦äºˆæ¸¬å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q supabase requests beautifulsoup4 lxml pandas numpy lightgbm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supabaseèªè¨¼æƒ…å ±ã®è¨­å®š\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    SUPABASE_URL = userdata.get('SUPABASE_URL') or \"https://khpoovshkhppadsirgcp.supabase.co\"\n",
    "    SUPABASE_KEY = userdata.get('SUPABASE_KEY')\n",
    "    if SUPABASE_KEY:\n",
    "        print(\"âœ“ Colabã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ\")\n",
    "    else:\n",
    "        raise Exception(\"SUPABASE_KEY not set\")\n",
    "except:\n",
    "    # ç›´æ¥å…¥åŠ›ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰\n",
    "    SUPABASE_URL = \"https://khpoovshkhppadsirgcp.supabase.co\"  # @param {type:\"string\"}\n",
    "    SUPABASE_KEY = \"your-service-role-key\"  # @param {type:\"string\"}\n",
    "    print(\"âš  æ‰‹å‹•å…¥åŠ›ã®èªè¨¼æƒ…å ±ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "\n",
    "# æ¥ç¶šãƒ†ã‚¹ãƒˆ\n",
    "from supabase import create_client\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "print(f\"âœ“ Supabaseæ¥ç¶š: {SUPABASE_URL[:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BUCKET_NAME = \"models\"\n",
    "RACE_TYPE = \"banei\"  # ã°ã‚“ãˆã„ç«¶é¦¬\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—\"\"\"\n",
    "    try:\n",
    "        files = supabase.storage.from_(BUCKET_NAME).list()\n",
    "        # ã°ã‚“ãˆã„ç”¨ãƒ¢ãƒ‡ãƒ«ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿\n",
    "        models = [f[\"name\"] for f in files if f[\"name\"].endswith(\".pkl\") and \"banei\" in f[\"name\"]]\n",
    "        print(f\"åˆ©ç”¨å¯èƒ½ãªã°ã‚“ãˆã„ãƒ¢ãƒ‡ãƒ«: {models}\")\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_model(version: str = \"v1\"):\n",
    "    \"\"\"ã°ã‚“ãˆã„ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\"\"\"\n",
    "    filename = f\"model_banei_{version}.pkl\"\n",
    "    print(f\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}\")\n",
    "\n",
    "    try:\n",
    "        response = supabase.storage.from_(BUCKET_NAME).download(filename)\n",
    "        buffer = io.BytesIO(response)\n",
    "        model_data = pickle.load(buffer)\n",
    "        print(f\"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ\")\n",
    "        print(f\"  ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {model_data.get('model_version', 'unknown')}\")\n",
    "        print(f\"  ç‰¹å¾´é‡æ•°: {len(model_data.get('feature_columns', []))}\")\n",
    "        return model_data\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º\n",
    "list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\nMODEL_VERSION = \"v20260126_105855\"  # @param {type:\"string\"}\n\nmodel_data = download_model(MODEL_VERSION)\n\nif model_data:\n    model = model_data[\"model\"]\n    scaler = model_data[\"scaler\"]\n    calibrator = model_data.get(\"calibrator\")\n    feature_columns = model_data[\"feature_columns\"]\n    print(f\"\\nâœ“ ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†ï¼ˆ{len(feature_columns)}ç‰¹å¾´é‡ï¼‰\")\n    print(f\"\\nç‰¹å¾´é‡ä¸€è¦§ï¼ˆå…ˆé ­10å€‹ï¼‰:\")\n    for col in feature_columns[:10]:\n        print(f\"  - {col}\")\nelse:\n    print(\"\\nâœ— ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n    print(\"ãƒ­ãƒ¼ã‚«ãƒ«FastAPIã§ã°ã‚“ãˆã„ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. å½“æ—¥ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆã°ã‚“ãˆã„ç«¶é¦¬ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–è¨­å®šï¼ˆColabç”¨ï¼‰\n",
    "USE_BROWSER = True  # @param {type:\"boolean\"} ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚ªãƒƒã‚ºã‚’å–å¾—\n",
    "\n",
    "if USE_BROWSER:\n",
    "    print(\"ãƒ–ãƒ©ã‚¦ã‚¶ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...\")\n",
    "    \n",
    "    # google-colab-selenium: Colabå°‚ç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
    "    !pip install -q google-colab-selenium\n",
    "    \n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    import google_colab_selenium as gs\n",
    "    \n",
    "    def get_chrome_driver():\n",
    "        \"\"\"Colabç”¨Chrome WebDriverã‚’å–å¾—\"\"\"\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920,1080\")\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "        \n",
    "        driver = gs.Chrome(options=options)\n",
    "        return driver\n",
    "    \n",
    "    # å‹•ä½œç¢ºèª\n",
    "    try:\n",
    "        print(\"  ChromeDriverã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...\")\n",
    "        test_driver = get_chrome_driver()\n",
    "        test_driver.get(\"https://www.google.com\")\n",
    "        title = test_driver.title\n",
    "        test_driver.quit()\n",
    "        print(f\"âœ“ ãƒ–ãƒ©ã‚¦ã‚¶æº–å‚™å®Œäº†ï¼ˆãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸: {title}ï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        USE_BROWSER = False\n",
    "\n",
    "# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š\n",
    "SCRAPE_INTERVAL = 1.5\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "# ã°ã‚“ãˆã„ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰\n",
    "BANEI_COURSE_CODE = \"65\"  # å¸¯åºƒ\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "last_request_time = 0\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    global last_request_time\n",
    "    elapsed = time.time() - last_request_time\n",
    "    if elapsed < SCRAPE_INTERVAL:\n",
    "        time.sleep(SCRAPE_INTERVAL - elapsed)\n",
    "\n",
    "    response = session.get(url, timeout=30)\n",
    "    last_request_time = time.time()\n",
    "\n",
    "    if \"EUC-JP\" in response.text[:500] or \"euc-jp\" in response.text[:500].lower():\n",
    "        response.encoding = \"euc-jp\"\n",
    "    else:\n",
    "        response.encoding = response.apparent_encoding or \"utf-8\"\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def is_banei_race(race_id: str) -> bool:\n",
    "    \"\"\"ã°ã‚“ãˆã„ç«¶é¦¬ã‹ã©ã†ã‹åˆ¤å®š\"\"\"\n",
    "    if len(race_id) >= 6:\n",
    "        return race_id[4:6] == BANEI_COURSE_CODE\n",
    "    return False\n",
    "\n",
    "print(\"âœ“ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_list_banei(target_date: date) -> List[Dict]:\n",
    "    \"\"\"æŒ‡å®šæ—¥ã®ã°ã‚“ãˆã„ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—ï¼ˆnar.netkeiba.comï¼‰\"\"\"\n",
    "    date_str = target_date.strftime(\"%Y%m%d\")\n",
    "    url = f\"https://nar.netkeiba.com/top/race_list_sub.html?kaisai_date={date_str}\"\n",
    "\n",
    "    html = fetch_html(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    races = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link.get(\"href\", \"\")\n",
    "        match = re.search(r\"race_id=(\\d{12})\", href)\n",
    "        if match:\n",
    "            race_id = match.group(1)\n",
    "            if race_id not in seen_ids:\n",
    "                # ã°ã‚“ãˆã„ç«¶é¦¬ã®ã¿\n",
    "                if not is_banei_race(race_id):\n",
    "                    continue\n",
    "                seen_ids.add(race_id)\n",
    "                races.append({\n",
    "                    \"race_id\": race_id,\n",
    "                    \"date\": target_date.isoformat(),\n",
    "                    \"race_name\": link.get_text(strip=True),\n",
    "                    \"course\": \"å¸¯åºƒ\",\n",
    "                })\n",
    "\n",
    "    races.sort(key=lambda x: x[\"race_id\"])\n",
    "    return races\n",
    "\n",
    "\n",
    "def get_odds_with_selenium_banei(race_id: str, driver, max_retries: int = 3, expected_count: int = 0) -> Dict[int, Dict]:\n",
    "    \"\"\"Seleniumã§ã°ã‚“ãˆã„ç«¶é¦¬ã®ã‚ªãƒƒã‚ºã‚’å–å¾—\"\"\"\n",
    "    url = f\"https://nar.netkeiba.com/odds/index.html?race_id={race_id}&type=b1\"\n",
    "    best_result = {}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            try:\n",
    "                WebDriverWait(driver, 20).until(\n",
    "                    lambda d: len(d.find_elements(By.CSS_SELECTOR, \"#odds_tan_block td.Odds span\")) > 0\n",
    "                )\n",
    "            except:\n",
    "                time.sleep(3.0)\n",
    "\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            result = {}\n",
    "            rows = driver.find_elements(By.CSS_SELECTOR, \"#odds_tan_block tr\")\n",
    "\n",
    "            for row in rows:\n",
    "                try:\n",
    "                    waku_elems = row.find_elements(By.CSS_SELECTOR, \"td[class^='Waku']\")\n",
    "                    if not waku_elems:\n",
    "                        continue\n",
    "                    umaban_text = waku_elems[0].text.strip()\n",
    "                    if not umaban_text.isdigit():\n",
    "                        continue\n",
    "                    umaban = int(umaban_text)\n",
    "\n",
    "                    odds_elem = row.find_elements(By.CSS_SELECTOR, \"td.Odds span\")\n",
    "                    if odds_elem:\n",
    "                        odds_text = odds_elem[0].text.strip()\n",
    "                        if odds_text and odds_text not in [\"---.-\", \"---\", \"\"]:\n",
    "                            result[umaban] = {\n",
    "                                \"odds\": float(odds_text),\n",
    "                                \"popularity\": None,\n",
    "                            }\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if len(result) > len(best_result):\n",
    "                best_result = result.copy()\n",
    "\n",
    "            if result and (expected_count == 0 or len(result) >= expected_count * 0.8):\n",
    "                return result\n",
    "\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  âš  ã‚ªãƒƒã‚ºå–å¾—ã‚¨ãƒ©ãƒ¼ã€ãƒªãƒˆãƒ©ã‚¤: {e}\")\n",
    "                time.sleep(2.5)\n",
    "\n",
    "    return best_result\n",
    "\n",
    "\n",
    "def scrape_shutuba_banei(race_id: str, driver=None) -> Dict:\n",
    "    \"\"\"ã°ã‚“ãˆã„ç«¶é¦¬ã®å‡ºé¦¬è¡¨ã‚’å–å¾—\"\"\"\n",
    "    url = f\"https://nar.netkeiba.com/race/shutuba.html?race_id={race_id}\"\n",
    "\n",
    "    response = session.get(url, timeout=30)\n",
    "    if \"EUC-JP\" in response.text[:500] or \"euc-jp\" in response.text[:500].lower():\n",
    "        response.encoding = \"euc-jp\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    race_data = {\n",
    "        \"race_id\": race_id,\n",
    "        \"course\": \"å¸¯åºƒ\",\n",
    "        \"race_number\": int(race_id[10:12]) if len(race_id) >= 12 else 0,\n",
    "        \"track_type\": \"ã°ã‚“ãˆã„\",\n",
    "        \"distance\": 200,  # ã°ã‚“ãˆã„ã¯200må›ºå®š\n",
    "    }\n",
    "\n",
    "    # ãƒ¬ãƒ¼ã‚¹å\n",
    "    title_elem = soup.select_one(\".RaceName\")\n",
    "    if title_elem:\n",
    "        race_data[\"race_name\"] = title_elem.get_text(strip=True)\n",
    "\n",
    "    # é¦¬å ´çŠ¶æ…‹ï¼ˆæ°´åˆ†é‡ï¼‰\n",
    "    race_data_elem = soup.select_one(\".RaceData01\")\n",
    "    if race_data_elem:\n",
    "        text = race_data_elem.get_text()\n",
    "        # æ°´åˆ†é‡ã‚’æŠ½å‡ºï¼ˆä¾‹: \"2.3%\"ï¼‰\n",
    "        moisture_match = re.search(r\"(\\d+\\.?\\d*)%\", text)\n",
    "        if moisture_match:\n",
    "            race_data[\"moisture\"] = float(moisture_match.group(1))\n",
    "\n",
    "    # ã‚°ãƒ¬ãƒ¼ãƒ‰\n",
    "    grade_elem = soup.select_one(\".RaceData02 span\")\n",
    "    if grade_elem:\n",
    "        race_data[\"grade\"] = grade_elem.get_text(strip=True)\n",
    "\n",
    "    # å‡ºèµ°é¦¬ã‚’å–å¾—\n",
    "    entries = []\n",
    "    for row in soup.select(\"tr.HorseList\"):\n",
    "        entry = parse_shutuba_row_banei(row)\n",
    "        if entry:\n",
    "            entries.append(entry)\n",
    "\n",
    "    field_size = len(entries)\n",
    "\n",
    "    # ã‚ªãƒƒã‚ºã‚’å–å¾—\n",
    "    odds_data = {}\n",
    "    if USE_BROWSER and driver:\n",
    "        odds_data = get_odds_with_selenium_banei(race_id, driver, expected_count=field_size)\n",
    "        if odds_data:\n",
    "            print(f\"  âœ“ {len(odds_data)}é ­ã®ã‚ªãƒƒã‚ºã‚’å–å¾—\")\n",
    "        else:\n",
    "            print(f\"  âš  ã‚ªãƒƒã‚ºå–å¾—å¤±æ•— - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨\")\n",
    "\n",
    "    # ã‚ªãƒƒã‚ºã‚’ãƒãƒ¼ã‚¸\n",
    "    for entry in entries:\n",
    "        horse_num = entry.get(\"horse_number\")\n",
    "        if horse_num and horse_num in odds_data:\n",
    "            entry[\"odds\"] = odds_data[horse_num].get(\"odds\")\n",
    "            entry[\"popularity\"] = odds_data[horse_num].get(\"popularity\")\n",
    "\n",
    "    race_data[\"entries\"] = entries\n",
    "    race_data[\"field_size\"] = field_size\n",
    "    return race_data\n",
    "\n",
    "\n",
    "def parse_shutuba_row_banei(row) -> Optional[Dict]:\n",
    "    \"\"\"ã°ã‚“ãˆã„ç«¶é¦¬ã®å‡ºé¦¬è¡¨è¡Œã‚’ãƒ‘ãƒ¼ã‚¹\"\"\"\n",
    "    entry = {}\n",
    "\n",
    "    tds = row.find_all(\"td\")\n",
    "\n",
    "    if len(tds) > 0:\n",
    "        try:\n",
    "            entry[\"frame_number\"] = int(tds[0].get_text(strip=True))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if len(tds) > 1:\n",
    "        try:\n",
    "            entry[\"horse_number\"] = int(tds[1].get_text(strip=True))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    horse_link = row.select_one(\"span.HorseName a\")\n",
    "    if horse_link:\n",
    "        href = horse_link.get(\"href\", \"\")\n",
    "        match = re.search(r\"/horse/(\\d+)\", href)\n",
    "        if match:\n",
    "            entry[\"horse_id\"] = match.group(1)\n",
    "        entry[\"horse_name\"] = horse_link.get_text(strip=True)\n",
    "\n",
    "    jockey_td = row.select_one(\"td.Jockey a\")\n",
    "    if jockey_td:\n",
    "        href = jockey_td.get(\"href\", \"\")\n",
    "        match = re.search(r\"/jockey/(?:result/recent/)?([a-zA-Z0-9]+)\", href)\n",
    "        if match:\n",
    "            entry[\"jockey_id\"] = match.group(1)\n",
    "        entry[\"jockey_name\"] = jockey_td.get_text(strip=True)\n",
    "\n",
    "    # ã‚½ãƒªé‡é‡ï¼ˆã°ã‚“ãˆã„ç‰¹æœ‰ï¼‰\n",
    "    if len(tds) > 5:\n",
    "        try:\n",
    "            entry[\"sori_weight\"] = float(tds[5].get_text(strip=True))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # é¦¬ä½“é‡\n",
    "    weight_td = row.select_one(\"td.Weight\")\n",
    "    if weight_td:\n",
    "        try:\n",
    "            weight_text = weight_td.get_text(strip=True)\n",
    "            weight_match = re.search(r\"(\\d+)\", weight_text)\n",
    "            if weight_match:\n",
    "                entry[\"horse_weight\"] = int(weight_match.group(1))\n",
    "            # ä½“é‡å¢—æ¸›\n",
    "            diff_match = re.search(r\"\\(([+-]?\\d+)\\)\", weight_text)\n",
    "            if diff_match:\n",
    "                entry[\"weight_diff\"] = int(diff_match.group(1))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return entry if entry.get(\"horse_number\") else None\n",
    "\n",
    "\n",
    "print(\"âœ“ ã°ã‚“ãˆã„ç«¶é¦¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¾è±¡æ—¥ã‚’è¨­å®š\n",
    "target_date = date.today()  # ä»Šæ—¥\n",
    "# target_date = date(2025, 1, 26)  # ç‰¹å®šã®æ—¥ä»˜\n",
    "\n",
    "print(f\"å¯¾è±¡æ—¥: {target_date}\")\n",
    "\n",
    "# ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—\n",
    "races = scrape_race_list_banei(target_date)\n",
    "print(f\"\\n{len(races)}ä»¶ã®ã°ã‚“ãˆã„ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ\")\n",
    "\n",
    "if races:\n",
    "    print(\"\\nã€å¸¯åºƒç«¶é¦¬å ´ã€‘\")\n",
    "    for race in races:\n",
    "        race_num = int(race[\"race_id\"][10:12])\n",
    "        print(f\"  {race_num}R: {race['race_name']}\")\n",
    "else:\n",
    "    print(\"\\næœ¬æ—¥ã¯ã°ã‚“ãˆã„ç«¶é¦¬ã®é–‹å‚¬ãŒãªã„ã‚ˆã†ã§ã™ã€‚\")\n",
    "    print(\"åˆ¥ã®æ—¥ä»˜ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. ç‰¹å¾´é‡ã®ç”Ÿæˆï¼ˆã°ã‚“ãˆã„å°‚ç”¨ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã°ã‚“ãˆã„ç”¨ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "BANEI_GRADE_MAP = {\n",
    "    \"æ–°é¦¬\": 0, \"æœªå‹åˆ©\": 1,\n",
    "    \"C4\": 2, \"C3\": 3, \"C2\": 4, \"C1\": 5,\n",
    "    \"B4\": 6, \"B3\": 7, \"B2\": 8, \"B1\": 9,\n",
    "    \"A2\": 10, \"A1\": 11,\n",
    "    \"ã‚ªãƒ¼ãƒ—ãƒ³\": 12, \"OP\": 12,\n",
    "    \"BG3\": 13, \"BG2\": 14, \"BG1\": 15,\n",
    "    \"ç‰¹åˆ¥\": 11,\n",
    "}\n",
    "\n",
    "# æ€§åˆ¥ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "SEX_MAP = {\"ç‰¡\": 0, \"ç‰\": 1, \"ã‚»\": 2}\n",
    "\n",
    "\n",
    "def create_features_banei(race_data: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ã°ã‚“ãˆã„ç«¶é¦¬ç”¨ã®ç‰¹å¾´é‡ã‚’ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    entries = race_data.get(\"entries\", [])\n",
    "    if not entries:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    features_list = []\n",
    "    field_size = race_data.get(\"field_size\", len(entries))\n",
    "    moisture = race_data.get(\"moisture\", 2.0)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ°´åˆ†é‡\n",
    "    grade = race_data.get(\"grade\", \"\")\n",
    "    grade_encoded = BANEI_GRADE_MAP.get(grade, -1)\n",
    "\n",
    "    # ã‚½ãƒªé‡é‡ã®å¹³å‡ã‚’è¨ˆç®—\n",
    "    sori_weights = [e.get(\"sori_weight\", 0) for e in entries if e.get(\"sori_weight\")]\n",
    "    avg_sori_weight = np.mean(sori_weights) if sori_weights else 0\n",
    "\n",
    "    # å­£ç¯€ç‰¹å¾´é‡\n",
    "    race_date = date.today()\n",
    "    month = race_date.month\n",
    "    if month in [3, 4, 5]:\n",
    "        season = 1  # æ˜¥\n",
    "    elif month in [6, 7, 8]:\n",
    "        season = 2  # å¤\n",
    "    elif month in [9, 10, 11]:\n",
    "        season = 3  # ç§‹\n",
    "    else:\n",
    "        season = 0  # å†¬\n",
    "    is_cold_season = 1 if month in [11, 12, 1, 2, 3] else 0\n",
    "\n",
    "    for i, entry in enumerate(entries):\n",
    "        sori_weight = entry.get(\"sori_weight\", 0) or 0\n",
    "        horse_weight = entry.get(\"horse_weight\", 900) or 900\n",
    "        odds = entry.get(\"odds\", 10) or 10\n",
    "\n",
    "        # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡ã«åˆã‚ã›ã‚‹ï¼‰\n",
    "        features = {\n",
    "            \"horse_number\": entry.get(\"horse_number\"),\n",
    "            \n",
    "            # IDç‰¹å¾´é‡\n",
    "            \"horse_id_int\": int(entry.get(\"horse_id\", \"0\")) if entry.get(\"horse_id\", \"\").isdigit() else 0,\n",
    "            \"jockey_id_int\": int(entry.get(\"jockey_id\", \"0\")) if entry.get(\"jockey_id\", \"\").isdigit() else 0,\n",
    "            \"trainer_id_int\": 0,  # å‡ºé¦¬è¡¨ã‹ã‚‰ã¯å–å¾—ä¸å¯\n",
    "            \"umaban\": entry.get(\"horse_number\", 0),\n",
    "            \n",
    "            # ã‚½ãƒªé‡é‡é–¢é€£\n",
    "            \"sori_weight\": sori_weight,\n",
    "            \"sori_weight_ratio\": sori_weight / horse_weight if horse_weight > 0 else 0,\n",
    "            \"sori_weight_rank\": 0,  # å¾Œã§è¨ˆç®—\n",
    "            \"sori_weight_normalized\": (sori_weight - avg_sori_weight) / 100 if avg_sori_weight > 0 else 0,\n",
    "            \"sori_weight_vs_class_avg\": sori_weight - (750 if \"A\" in grade else 680 if \"B\" in grade else 620 if \"C\" in grade else 580),\n",
    "            \n",
    "            # é¦¬ä½“é‡é–¢é€£\n",
    "            \"horse_weight_banei\": horse_weight,\n",
    "            \"weight_diff_banei\": entry.get(\"weight_diff\", 0) or 0,\n",
    "            \"weight_trend\": 0,  # éå»ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚0\n",
    "            \"power_index\": horse_weight / (sori_weight + 1) if sori_weight > 0 else 0,\n",
    "            \"optimal_weight_gap\": 0,  # éå»ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚0\n",
    "            \n",
    "            # æ°´åˆ†é‡ãƒ»é¦¬å ´\n",
    "            \"moisture_level\": moisture,\n",
    "            \"moisture_aptitude\": 0,  # éå»ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚0\n",
    "            \"is_light_track\": 1 if moisture < 2.0 else 0,\n",
    "            \"is_heavy_track\": 1 if moisture > 4.0 else 0,\n",
    "            \n",
    "            # ãƒ¬ãƒ¼ã‚¹æ¡ä»¶\n",
    "            \"grade_encoded\": grade_encoded,\n",
    "            \"race_number\": race_data.get(\"race_number\", 1),\n",
    "            \"field_size\": field_size,\n",
    "            \"frame_number\": entry.get(\"frame_number\", 0) or 0,\n",
    "            \n",
    "            # é¦¬åŸºæœ¬æƒ…å ±ï¼ˆå‡ºé¦¬è¡¨ã‹ã‚‰ã¯å–å¾—å›°é›£ãªã®ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰\n",
    "            \"age\": 5,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ\n",
    "            \"age_category\": 1,  # ä¸­å …\n",
    "            \"sex\": 0,  # ç‰¡\n",
    "            \"sex_weight_bonus\": 0,\n",
    "            \n",
    "            # é¨æ‰‹ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰\n",
    "            \"jockey_win_rate\": 0.1,\n",
    "            \"jockey_place_rate\": 0.2,\n",
    "            \"jockey_year_rank\": 10,\n",
    "            \"jockey_heavy_win_rate\": 0.1,\n",
    "            \"jockey_moisture_apt\": 0.2,\n",
    "            \"jockey_horse_combo\": 0,\n",
    "            \n",
    "            # éå»æˆç¸¾ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰\n",
    "            \"avg_rank_last3\": 5,\n",
    "            \"avg_rank_last5\": 5,\n",
    "            \"win_rate\": 0.1,\n",
    "            \"place_rate\": 0.2,\n",
    "            \"show_rate\": 0.3,\n",
    "            \"days_since_last\": 14,\n",
    "            \"last_result\": 5,\n",
    "            \"best_rank\": 3,\n",
    "            \n",
    "            # é‡é‡åˆ¥æˆç¸¾\n",
    "            \"light_weight_win_rate\": 0.1,\n",
    "            \"mid_weight_win_rate\": 0.1,\n",
    "            \"heavy_weight_win_rate\": 0.1,\n",
    "            \n",
    "            # ã‚ªãƒƒã‚º\n",
    "            \"odds\": odds,\n",
    "            \"log_odds\": np.log1p(odds),\n",
    "            \"popularity\": entry.get(\"popularity\", 5) or 5,\n",
    "            \n",
    "            # å­£ç¯€\n",
    "            \"month\": month,\n",
    "            \"season\": season,\n",
    "            \"is_cold_season\": is_cold_season,\n",
    "        }\n",
    "\n",
    "        features_list.append(features)\n",
    "\n",
    "    df = pd.DataFrame(features_list)\n",
    "\n",
    "    # ã‚½ãƒªé‡é‡é †ä½ã‚’è¨ˆç®—\n",
    "    df[\"sori_weight_rank\"] = df[\"sori_weight\"].rank(ascending=False).astype(int)\n",
    "\n",
    "    # è¶³ã‚Šãªã„ç‰¹å¾´é‡ã‚’0ã§åŸ‹ã‚ã‚‹\n",
    "    for col in feature_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"âœ“ ã°ã‚“ãˆã„ç‰¹å¾´é‡ç”Ÿæˆé–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. äºˆæ¸¬ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_race_banei(race_data: Dict) -> Dict:\n",
    "    \"\"\"ã°ã‚“ãˆã„ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ\"\"\"\n",
    "    df = create_features_banei(race_data)\n",
    "\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    # ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "    X = df[feature_columns].copy()\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "    X_scaled = pd.DataFrame(\n",
    "        scaler.transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index,\n",
    "    )\n",
    "\n",
    "    # äºˆæ¸¬\n",
    "    probs = model.predict(X_scaled)\n",
    "\n",
    "    # ç¢ºç‡ã®æ­£è¦åŒ–\n",
    "    total = probs.sum()\n",
    "    if total > 0:\n",
    "        probabilities = probs / total\n",
    "    else:\n",
    "        probabilities = probs\n",
    "\n",
    "    # çµæœã‚’æ•´å½¢\n",
    "    results = []\n",
    "    entries = race_data.get(\"entries\", [])\n",
    "\n",
    "    for i, entry in enumerate(entries):\n",
    "        odds = entry.get(\"odds\") or 10\n",
    "        prob = float(probabilities[i])\n",
    "        ev = prob * odds\n",
    "\n",
    "        results.append({\n",
    "            \"horse_number\": entry.get(\"horse_number\"),\n",
    "            \"horse_name\": entry.get(\"horse_name\", \"ä¸æ˜\"),\n",
    "            \"jockey_name\": entry.get(\"jockey_name\", \"\"),\n",
    "            \"sori_weight\": entry.get(\"sori_weight\"),\n",
    "            \"horse_weight\": entry.get(\"horse_weight\"),\n",
    "            \"score\": float(probs[i]),\n",
    "            \"probability\": prob,\n",
    "            \"odds\": odds,\n",
    "            \"expected_value\": ev,\n",
    "            \"popularity\": entry.get(\"popularity\"),\n",
    "        })\n",
    "\n",
    "    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    # é †ä½ä»˜ã‘\n",
    "    for i, r in enumerate(results):\n",
    "        r[\"pred_rank\"] = i + 1\n",
    "\n",
    "    return {\n",
    "        \"race_id\": race_data[\"race_id\"],\n",
    "        \"race_name\": race_data.get(\"race_name\", \"\"),\n",
    "        \"course\": race_data.get(\"course\", \"å¸¯åºƒ\"),\n",
    "        \"race_number\": race_data.get(\"race_number\"),\n",
    "        \"moisture\": race_data.get(\"moisture\"),\n",
    "        \"grade\": race_data.get(\"grade\"),\n",
    "        \"predictions\": results,\n",
    "    }\n",
    "\n",
    "print(\"âœ“ äºˆæ¸¬é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’è¡¨ç¤º\nimport unicodedata\n\ndef get_east_asian_width(text):\n    \"\"\"æ–‡å­—åˆ—ã®è¡¨ç¤ºå¹…ã‚’è¨ˆç®—ï¼ˆå…¨è§’=2, åŠè§’=1ï¼‰\"\"\"\n    width = 0\n    for char in str(text):\n        if unicodedata.east_asian_width(char) in 'FWA':\n            width += 2\n        else:\n            width += 1\n    return width\n\ndef pad_to_width(text, width):\n    \"\"\"æŒ‡å®šå¹…ã«ãªã‚‹ã‚ˆã†ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\"\"\"\n    text = str(text)\n    current_width = get_east_asian_width(text)\n    padding = width - current_width\n    return text + ' ' * max(0, padding)\n\nprint(\"=== æœ¬æ—¥ã®ã°ã‚“ãˆã„ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ ===\")\nprint()\nprint(f\"  No.  å ´æ‰€      ãƒ¬ãƒ¼ã‚¹å\")\nprint(\"-\" * 50)\nfor i, race in enumerate(races):\n    race_num = int(race[\"race_id\"][10:12])\n    race_name = race['race_name'][:15] if len(race['race_name']) > 15 else race['race_name']\n    name_padded = pad_to_width(race_name, 30)\n    print(f\"  {i+1:2d}.  å¸¯åºƒ{race_num:2d}R  {name_padded}  (ID: {race['race_id']})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬\n\nimport unicodedata\n\ndef get_east_asian_width(text):\n    \"\"\"æ–‡å­—åˆ—ã®è¡¨ç¤ºå¹…ã‚’è¨ˆç®—ï¼ˆå…¨è§’=2, åŠè§’=1ï¼‰\"\"\"\n    width = 0\n    for char in str(text):\n        if unicodedata.east_asian_width(char) in 'FWA':\n            width += 2\n        else:\n            width += 1\n    return width\n\ndef pad_to_width(text, width):\n    \"\"\"æŒ‡å®šå¹…ã«ãªã‚‹ã‚ˆã†ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\"\"\"\n    text = str(text)\n    current_width = get_east_asian_width(text)\n    padding = width - current_width\n    return text + ' ' * max(0, padding)\n\n# æ¨å¥¨é¦¬ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶\nEV_THRESHOLD = 1.0   # æœŸå¾…å€¤ä¸‹é™\nMAX_EV = 2.0         # æœŸå¾…å€¤ä¸Šé™ï¼ˆé«˜ã™ãã‚‹ç©´é¦¬ã‚’é™¤å¤–ï¼‰\nMIN_PROB = 0.05      # æœ€ä½ç¢ºç‡5%\nTOP_N_PER_RACE = 3   # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®æœ€å¤§æ¨å¥¨æ•°\n\n# ä¸Šã®ä¸€è¦§ã‹ã‚‰ç•ªå·ã‚’æŒ‡å®šã™ã‚‹ã‹ã€race_idã‚’ç›´æ¥å…¥åŠ›\nRACE_INDEX = 1  # @param {type:\"integer\"} ä¸€è¦§ã®ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰\n# ã¾ãŸã¯ç›´æ¥race_idã‚’æŒ‡å®šï¼ˆç©ºæ¬„ã®å ´åˆã¯ä¸Šã®ç•ªå·ã‚’ä½¿ç”¨ï¼‰\nRACE_ID = \"\"  # @param {type:\"string\"}\n\n# WebDriverã‚’åˆæœŸåŒ–\ndriver = None\nif USE_BROWSER:\n    try:\n        driver = get_chrome_driver()\n    except Exception as e:\n        print(f\"âš  ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}\")\n\n# ãƒ¬ãƒ¼ã‚¹ã‚’ç‰¹å®š\nif RACE_ID:\n    target_race_id = RACE_ID\nelse:\n    if races and 1 <= RACE_INDEX <= len(races):\n        target_race_id = races[RACE_INDEX - 1][\"race_id\"]\n    else:\n        print(f\"ã‚¨ãƒ©ãƒ¼: ç•ªå·ã¯1ã€œ{len(races) if races else 0}ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„\")\n        target_race_id = None\n\nif target_race_id:\n    print(f\"äºˆæ¸¬å¯¾è±¡: {target_race_id}\")\n    print(f\"ç¾åœ¨æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}\")\n    print(\"-\" * 70)\n    \n    try:\n        # å‡ºé¦¬è¡¨ã¨ã‚ªãƒƒã‚ºã‚’å–å¾—\n        print(\"\\nå‡ºé¦¬è¡¨ã¨ã‚ªãƒƒã‚ºã‚’å–å¾—ä¸­...\")\n        race_data = scrape_shutuba_banei(target_race_id, driver=driver)\n        \n        # ã‚ªãƒƒã‚ºã‹ã‚‰äººæ°—ã‚’ç®—å‡º\n        entries_with_odds = [(i, e.get(\"odds\", 999)) for i, e in enumerate(race_data.get(\"entries\", []))]\n        entries_with_odds.sort(key=lambda x: x[1])\n        for rank, (idx, _) in enumerate(entries_with_odds, 1):\n            race_data[\"entries\"][idx][\"popularity\"] = rank\n        \n        # äºˆæ¸¬å®Ÿè¡Œ\n        pred = predict_race_banei(race_data)\n        \n        if pred:\n            # çµæœè¡¨ç¤º\n            moisture = pred.get('moisture')\n            moisture_str = f\"{moisture}%\" if moisture is not None else \"-\"\n            \n            print(f\"\\n{'='*70}\")\n            print(f\"å¸¯åºƒ {pred.get('race_number', '')}R {pred['race_name']}\")\n            print(f\"ã‚°ãƒ¬ãƒ¼ãƒ‰: {pred.get('grade') or '-'}  æ°´åˆ†é‡: {moisture_str}\")\n            print(f\"{'='*70}\")\n            print(f\"  äºˆæƒ³  é¦¬ç•ª  é¦¬å                  å‹ç‡    ã‚ªãƒƒã‚º  æœŸå¾…å€¤  äººæ°—\")\n            print(\"-\" * 70)\n            \n            for p in pred[\"predictions\"]:\n                prob_pct = (p[\"probability\"] or 0) * 100\n                odds = p.get('odds')\n                odds_str = f\"{odds:.1f}\" if odds else \"-\"\n                ev = p.get(\"expected_value\") or 0\n                pop = p.get(\"popularity\") or \"-\"\n                name_padded = pad_to_width(p['horse_name'], 18)\n                \n                # æ¨å¥¨ãƒãƒ¼ã‚¯\n                if EV_THRESHOLD <= ev <= MAX_EV and (p.get(\"probability\") or 0) >= MIN_PROB:\n                    mark = \"â˜…\"\n                else:\n                    mark = \"\"\n                \n                print(f\"  {p['pred_rank']:>4}  {p['horse_number']:>4}  {name_padded}  {prob_pct:>5.1f}%  {odds_str:>6}  {ev:>5.2f}{mark:<2} {pop:>4}\")\n            \n            # æ¨å¥¨é¦¬ã®ã¿æŠ½å‡º\n            recommended = [p for p in pred[\"predictions\"] \n                           if EV_THRESHOLD <= (p.get(\"expected_value\") or 0) <= MAX_EV \n                           and (p.get(\"probability\") or 0) >= MIN_PROB]\n            if recommended:\n                print(f\"\\nã€æ¨å¥¨é¦¬ï¼ˆEV {EV_THRESHOLD}ã€œ{MAX_EV}ã€ç¢ºç‡{MIN_PROB*100:.0f}%ä»¥ä¸Šï¼‰ã€‘\")\n                for p in sorted(recommended, key=lambda x: x[\"expected_value\"], reverse=True)[:TOP_N_PER_RACE]:\n                    print(f\"  é¦¬ç•ª{p['horse_number']:2d} {p['horse_name']}: EV={p['expected_value']:.2f}, ã‚ªãƒƒã‚º={p['odds']:.1f}\")\n        else:\n            print(\"äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n    \n    except Exception as e:\n        print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n    finally:\n        if driver:\n            driver.quit()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ä½¿ã„æ–¹ã¾ã¨ã‚\n",
    "\n",
    "### äº‹å‰æº–å‚™ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«PCã§å®Ÿè¡Œï¼‰\n",
    "\n",
    "1. **ã°ã‚“ãˆã„ç”¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’**\n",
    "   - ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®ã°ã‚“ãˆã„ > ãƒ¢ãƒ‡ãƒ«ç®¡ç†ç”»é¢ã§ã€Œå†å­¦ç¿’ã‚’é–‹å§‹ã€\n",
    "   - ã¾ãŸã¯ API: `POST /api/v1/model/retrain` with `{\"race_type\": \"banei\", ...}`\n",
    "\n",
    "2. **ãƒ¢ãƒ‡ãƒ«ã‚’Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**\n",
    "   - `POST /api/v1/model/storage/upload?race_type=banei`\n",
    "\n",
    "### å½“æ—¥äºˆæ¸¬ï¼ˆã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼‰\n",
    "\n",
    "1. ã‚»ãƒ«1ã€œ5ã‚’é †ç•ªã«å®Ÿè¡Œï¼ˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€œäºˆæ¸¬é–¢æ•°å®šç¾©ï¼‰\n",
    "2. ã‚»ãƒ«6ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã§å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ã®ç•ªå·ã‚’ç¢ºèª\n",
    "3. `RACE_INDEX` ã‚’å¤‰æ›´ã—ã¦äºˆæ¸¬å®Ÿè¡Œ\n",
    "\n",
    "### ã°ã‚“ãˆã„ç«¶é¦¬ã®ãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "- **ã‚½ãƒªé‡é‡**: é‡ã„ã»ã©ä¸åˆ©ã ãŒã€å¼·ã„é¦¬ã¯é‡ã„ã‚½ãƒªã§ã‚‚å‹ã¤\n",
    "- **æ°´åˆ†é‡**: é¦¬å ´ã®æ°´åˆ†é‡ï¼ˆ%ï¼‰ãŒé‡è¦ã€è»½ã„é¦¬å ´ã¨é‡ã„é¦¬å ´ã§å¾—æ„ä¸å¾—æ„ãŒã‚ã‚‹\n",
    "- **é¨æ‰‹**: é€šå¸¸ç«¶é¦¬ã‚ˆã‚Šé¨æ‰‹ã®å½±éŸ¿åº¦ãŒé«˜ã„ï¼ˆé¦¬3:é¨æ‰‹7ï¼‰\n",
    "- **å­£ç¯€**: å†¬å­£ï¼ˆ11æœˆã€œ3æœˆï¼‰ã¯é¦¬å ´çŠ¶æ…‹ãŒå¤§ããå¤‰ã‚ã‚‹\n",
    "\n",
    "### é–‹å‚¬æ—¥ç¨‹\n",
    "\n",
    "- å¸¯åºƒç«¶é¦¬å ´ã§é€±4ã€œ5æ—¥é–‹å‚¬\n",
    "- ä¸»ã«åœŸæ—¥æœˆ+å¹³æ—¥1ã€œ2æ—¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}