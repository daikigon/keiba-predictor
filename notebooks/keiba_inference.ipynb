{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒª - æ¨è«–å°‚ç”¨ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯**æ¨è«–ï¼ˆäºˆæ¸¬ï¼‰ã®ã¿**ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "## ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆ\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ãƒ­ãƒ¼ã‚«ãƒ«FastAPI â”‚     â”‚   Supabase       â”‚\n",
    "â”‚  - ãƒ¢ãƒ‡ãƒ«å­¦ç¿’    â”‚â”€â”€â”€â”€â–¶â”‚  - ãƒ¢ãƒ‡ãƒ«ä¿å­˜    â”‚\n",
    "â”‚  - ç‰¹å¾´é‡ç”Ÿæˆ    â”‚     â”‚  - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                        â”‚   Google Colab   â”‚\n",
    "                        â”‚  - ãƒ¢ãƒ‡ãƒ«èª­è¾¼    â”‚ â† ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "                        â”‚  - å½“æ—¥äºˆæ¸¬å®Ÿè¡Œ  â”‚\n",
    "                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ä½¿ç”¨æ–¹æ³•\n",
    "1. å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ğŸ”‘ã‹ã‚‰ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã«ä»¥ä¸‹ã‚’è¨­å®šï¼š\n",
    "   - `SUPABASE_URL`\n",
    "   - `SUPABASE_KEY`\n",
    "2. ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ\n",
    "\n",
    "## å‰ææ¡ä»¶\n",
    "- ãƒ­ãƒ¼ã‚«ãƒ«FastAPIã§å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒSupabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨\n",
    "- Supabase Storageã« `models` ãƒã‚±ãƒƒãƒˆãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q supabase requests beautifulsoup4 lxml pandas numpy lightgbm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supabaseèªè¨¼æƒ…å ±ã®è¨­å®š\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    SUPABASE_URL = userdata.get('SUPABASE_URL')\n",
    "    SUPABASE_KEY = userdata.get('SUPABASE_KEY')\n",
    "    print(\"Colabã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ\")\n",
    "except:\n",
    "    # ç›´æ¥å…¥åŠ›ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰\n",
    "    SUPABASE_URL = \"https://your-project.supabase.co\"  # @param {type:\"string\"}\n",
    "    SUPABASE_KEY = \"your-service-role-key\"  # @param {type:\"string\"}\n",
    "    print(\"æ‰‹å‹•å…¥åŠ›ã®èªè¨¼æƒ…å ±ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "\n",
    "# æ¥ç¶šãƒ†ã‚¹ãƒˆ\n",
    "from supabase import create_client\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "print(f\"Supabaseæ¥ç¶š: {SUPABASE_URL[:30]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BUCKET_NAME = \"models\"\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—\"\"\"\n",
    "    try:\n",
    "        files = supabase.storage.from_(BUCKET_NAME).list()\n",
    "        models = [f[\"name\"] for f in files if f[\"name\"].endswith(\".pkl\")]\n",
    "        print(f\"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {models}\")\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_model(version: str = \"v1\"):\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\"\"\"\n",
    "    filename = f\"model_{version}.pkl\"\n",
    "    print(f\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        response = supabase.storage.from_(BUCKET_NAME).download(filename)\n",
    "        buffer = io.BytesIO(response)\n",
    "        model_data = pickle.load(buffer)\n",
    "        print(f\"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ\")\n",
    "        print(f\"  ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {model_data.get('model_version', 'unknown')}\")\n",
    "        print(f\"  ç‰¹å¾´é‡æ•°: {len(model_data.get('feature_columns', []))}\")\n",
    "        return model_data\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º\n",
    "list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "MODEL_VERSION = \"v1\"  # @param {type:\"string\"}\n",
    "\n",
    "model_data = download_model(MODEL_VERSION)\n",
    "\n",
    "if model_data:\n",
    "    model = model_data[\"model\"]\n",
    "    scaler = model_data[\"scaler\"]\n",
    "    calibrator = model_data.get(\"calibrator\")\n",
    "    feature_columns = model_data[\"feature_columns\"]\n",
    "    print(f\"\\nãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†\")\n",
    "else:\n",
    "    print(\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    print(\"ãƒ­ãƒ¼ã‚«ãƒ«FastAPIã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å½“æ—¥ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š\n",
    "SCRAPE_INTERVAL = 1.5\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "JRA_COURSE_CODES = {\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"}\n",
    "COURSE_NAMES = {\n",
    "    \"01\": \"æœ­å¹Œ\", \"02\": \"å‡½é¤¨\", \"03\": \"ç¦å³¶\", \"04\": \"æ–°æ½Ÿ\",\n",
    "    \"05\": \"æ±äº¬\", \"06\": \"ä¸­å±±\", \"07\": \"ä¸­äº¬\", \"08\": \"äº¬éƒ½\",\n",
    "    \"09\": \"é˜ªç¥\", \"10\": \"å°å€‰\",\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "last_request_time = 0\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    global last_request_time\n",
    "    elapsed = time.time() - last_request_time\n",
    "    if elapsed < SCRAPE_INTERVAL:\n",
    "        time.sleep(SCRAPE_INTERVAL - elapsed)\n",
    "    \n",
    "    response = session.get(url, timeout=30)\n",
    "    last_request_time = time.time()\n",
    "    \n",
    "    if \"EUC-JP\" in response.text[:500] or \"euc-jp\" in response.text[:500].lower():\n",
    "        response.encoding = \"euc-jp\"\n",
    "    else:\n",
    "        response.encoding = response.apparent_encoding or \"utf-8\"\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "def is_jra_race(race_id: str) -> bool:\n",
    "    if len(race_id) >= 6:\n",
    "        return race_id[4:6] in JRA_COURSE_CODES\n",
    "    return False\n",
    "\n",
    "print(\"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_list(target_date: date, jra_only: bool = True) -> List[Dict]:\n",
    "    \"\"\"æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—\"\"\"\n",
    "    date_str = target_date.strftime(\"%Y%m%d\")\n",
    "    url = f\"https://race.netkeiba.com/top/race_list.html?kaisai_date={date_str}\"\n",
    "    \n",
    "    html = fetch_html(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    races = []\n",
    "    seen_ids = set()\n",
    "    \n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link.get(\"href\", \"\")\n",
    "        match = re.search(r\"race_id=(\\d{12})\", href)\n",
    "        if match:\n",
    "            race_id = match.group(1)\n",
    "            if race_id not in seen_ids:\n",
    "                if jra_only and not is_jra_race(race_id):\n",
    "                    continue\n",
    "                seen_ids.add(race_id)\n",
    "                races.append({\n",
    "                    \"race_id\": race_id,\n",
    "                    \"date\": target_date.isoformat(),\n",
    "                    \"race_name\": link.get_text(strip=True),\n",
    "                })\n",
    "    \n",
    "    return races\n",
    "\n",
    "def scrape_shutuba(race_id: str) -> Dict:\n",
    "    \"\"\"å‡ºé¦¬è¡¨ã‚’å–å¾—ï¼ˆãƒ¬ãƒ¼ã‚¹å‰ã®ãƒ‡ãƒ¼ã‚¿ï¼‰\"\"\"\n",
    "    url = f\"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}\"\n",
    "    html = fetch_html(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    race_data = {\n",
    "        \"race_id\": race_id,\n",
    "        \"course\": COURSE_NAMES.get(race_id[4:6], \"\"),\n",
    "        \"race_number\": int(race_id[10:12]) if len(race_id) >= 12 else 0,\n",
    "    }\n",
    "    \n",
    "    # ãƒ¬ãƒ¼ã‚¹å\n",
    "    title_elem = soup.select_one(\".RaceName\")\n",
    "    if title_elem:\n",
    "        race_data[\"race_name\"] = title_elem.get_text(strip=True)\n",
    "    \n",
    "    # è·é›¢ãƒ»é¦¬å ´\n",
    "    race_data_elem = soup.select_one(\".RaceData01\")\n",
    "    if race_data_elem:\n",
    "        text = race_data_elem.get_text()\n",
    "        \n",
    "        distance_match = re.search(r\"(\\d+)m\", text)\n",
    "        if distance_match:\n",
    "            race_data[\"distance\"] = int(distance_match.group(1))\n",
    "        \n",
    "        if \"èŠ\" in text:\n",
    "            race_data[\"track_type\"] = \"èŠ\"\n",
    "        elif \"ãƒ€ãƒ¼ãƒˆ\" in text or \"ãƒ€\" in text:\n",
    "            race_data[\"track_type\"] = \"ãƒ€ãƒ¼ãƒˆ\"\n",
    "    \n",
    "    # å‡ºèµ°é¦¬\n",
    "    entries = []\n",
    "    table = soup.select_one(\"table.Shutuba_Table\")\n",
    "    if table:\n",
    "        for row in table.select(\"tr.HorseList\"):\n",
    "            entry = parse_shutuba_row(row)\n",
    "            if entry:\n",
    "                entries.append(entry)\n",
    "    \n",
    "    race_data[\"entries\"] = entries\n",
    "    return race_data\n",
    "\n",
    "def parse_shutuba_row(row) -> Optional[Dict]:\n",
    "    \"\"\"å‡ºé¦¬è¡¨ã®è¡Œã‚’ãƒ‘ãƒ¼ã‚¹\"\"\"\n",
    "    entry = {}\n",
    "    \n",
    "    # æ ç•ª\n",
    "    waku = row.select_one(\"td.Waku\")\n",
    "    if waku:\n",
    "        try:\n",
    "            entry[\"frame_number\"] = int(waku.get_text(strip=True))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # é¦¬ç•ª\n",
    "    umaban = row.select_one(\"td.Umaban\")\n",
    "    if umaban:\n",
    "        try:\n",
    "            entry[\"horse_number\"] = int(umaban.get_text(strip=True))\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # é¦¬åãƒ»ID\n",
    "    horse_link = row.select_one(\"span.HorseName a\")\n",
    "    if horse_link:\n",
    "        href = horse_link.get(\"href\", \"\")\n",
    "        match = re.search(r\"/horse/(\\d+)\", href)\n",
    "        if match:\n",
    "            entry[\"horse_id\"] = match.group(1)\n",
    "        entry[\"horse_name\"] = horse_link.get_text(strip=True)\n",
    "    \n",
    "    # é¨æ‰‹\n",
    "    jockey_link = row.select_one(\"td.Jockey a\")\n",
    "    if jockey_link:\n",
    "        href = jockey_link.get(\"href\", \"\")\n",
    "        match = re.search(r\"/jockey/(?:result/recent/)?(\\d+)\", href)\n",
    "        if match:\n",
    "            entry[\"jockey_id\"] = match.group(1)\n",
    "        entry[\"jockey_name\"] = jockey_link.get_text(strip=True)\n",
    "    \n",
    "    # æ–¤é‡\n",
    "    weight_elem = row.select_one(\"td.Txt_C\")\n",
    "    if weight_elem:\n",
    "        try:\n",
    "            entry[\"weight\"] = float(weight_elem.get_text(strip=True))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # ã‚ªãƒƒã‚º\n",
    "    odds_elem = row.select_one(\"td.Odds span\")\n",
    "    if odds_elem:\n",
    "        try:\n",
    "            odds_text = odds_elem.get_text(strip=True)\n",
    "            if odds_text and odds_text != \"---\":\n",
    "                entry[\"odds\"] = float(odds_text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return entry if entry.get(\"horse_number\") else None\n",
    "\n",
    "print(\"å‡ºé¦¬è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¾è±¡æ—¥ã‚’è¨­å®š\n",
    "target_date = date.today()  # ä»Šæ—¥\n",
    "# target_date = date(2024, 12, 28)  # ç‰¹å®šã®æ—¥ä»˜\n",
    "\n",
    "print(f\"å¯¾è±¡æ—¥: {target_date}\")\n",
    "\n",
    "# ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—\n",
    "races = scrape_race_list(target_date, jra_only=True)\n",
    "print(f\"\\n{len(races)}ä»¶ã®JRAãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ\")\n",
    "\n",
    "for race in races[:5]:\n",
    "    print(f\"  - {race['race_id']}: {race['race_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç‰¹å¾´é‡ã®ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_horse_stats(horse_id: str) -> Dict:\n",
    "    \"\"\"é¦¬ã®éå»æˆç¸¾ã‚’å–å¾—ï¼ˆSupabaseã‹ã‚‰ï¼‰\"\"\"\n",
    "    try:\n",
    "        result = supabase.table(\"entries\").select(\"result, odds\").eq(\"horse_id\", horse_id).order(\"id\", desc=True).limit(10).execute()\n",
    "        \n",
    "        if not result.data:\n",
    "            return {\"win_rate\": 0, \"place_rate\": 0, \"avg_odds\": 10}\n",
    "        \n",
    "        results = [r[\"result\"] for r in result.data if r[\"result\"]]\n",
    "        odds_list = [r[\"odds\"] for r in result.data if r[\"odds\"]]\n",
    "        \n",
    "        if not results:\n",
    "            return {\"win_rate\": 0, \"place_rate\": 0, \"avg_odds\": 10}\n",
    "        \n",
    "        win_rate = sum(1 for r in results if r == 1) / len(results)\n",
    "        place_rate = sum(1 for r in results if r <= 3) / len(results)\n",
    "        avg_odds = sum(odds_list) / len(odds_list) if odds_list else 10\n",
    "        \n",
    "        return {\n",
    "            \"win_rate\": win_rate,\n",
    "            \"place_rate\": place_rate,\n",
    "            \"avg_odds\": avg_odds,\n",
    "        }\n",
    "    except:\n",
    "        return {\"win_rate\": 0, \"place_rate\": 0, \"avg_odds\": 10}\n",
    "\n",
    "def get_jockey_stats(jockey_id: str) -> Dict:\n",
    "    \"\"\"é¨æ‰‹ã®éå»æˆç¸¾ã‚’å–å¾—ï¼ˆSupabaseã‹ã‚‰ï¼‰\"\"\"\n",
    "    try:\n",
    "        result = supabase.table(\"entries\").select(\"result\").eq(\"jockey_id\", jockey_id).order(\"id\", desc=True).limit(50).execute()\n",
    "        \n",
    "        if not result.data:\n",
    "            return {\"jockey_win_rate\": 0, \"jockey_place_rate\": 0}\n",
    "        \n",
    "        results = [r[\"result\"] for r in result.data if r[\"result\"]]\n",
    "        \n",
    "        if not results:\n",
    "            return {\"jockey_win_rate\": 0, \"jockey_place_rate\": 0}\n",
    "        \n",
    "        win_rate = sum(1 for r in results if r == 1) / len(results)\n",
    "        place_rate = sum(1 for r in results if r <= 3) / len(results)\n",
    "        \n",
    "        return {\n",
    "            \"jockey_win_rate\": win_rate,\n",
    "            \"jockey_place_rate\": place_rate,\n",
    "        }\n",
    "    except:\n",
    "        return {\"jockey_win_rate\": 0, \"jockey_place_rate\": 0}\n",
    "\n",
    "print(\"çµ±è¨ˆå–å¾—é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(race_data: Dict) -> pd.DataFrame:\n",
    "    \"\"\"ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’ç”Ÿæˆ\"\"\"\n",
    "    entries = race_data.get(\"entries\", [])\n",
    "    if not entries:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for entry in entries:\n",
    "        features = {\n",
    "            \"horse_number\": entry.get(\"horse_number\"),\n",
    "            \"frame_number\": entry.get(\"frame_number\", 0),\n",
    "            \"weight\": entry.get(\"weight\", 55),\n",
    "            \"odds\": entry.get(\"odds\", 10),\n",
    "            \"distance\": race_data.get(\"distance\", 1600),\n",
    "            \"track_type\": 1 if race_data.get(\"track_type\") == \"èŠ\" else 0,\n",
    "        }\n",
    "        \n",
    "        # é¦¬ã®éå»æˆç¸¾\n",
    "        if entry.get(\"horse_id\"):\n",
    "            horse_stats = get_horse_stats(entry[\"horse_id\"])\n",
    "            features.update(horse_stats)\n",
    "        else:\n",
    "            features.update({\"win_rate\": 0, \"place_rate\": 0, \"avg_odds\": 10})\n",
    "        \n",
    "        # é¨æ‰‹ã®éå»æˆç¸¾\n",
    "        if entry.get(\"jockey_id\"):\n",
    "            jockey_stats = get_jockey_stats(entry[\"jockey_id\"])\n",
    "            features.update(jockey_stats)\n",
    "        else:\n",
    "            features.update({\"jockey_win_rate\": 0, \"jockey_place_rate\": 0})\n",
    "        \n",
    "        features_list.append(features)\n",
    "    \n",
    "    df = pd.DataFrame(features_list)\n",
    "    \n",
    "    # è¶³ã‚Šãªã„ç‰¹å¾´é‡ã‚’0ã§åŸ‹ã‚ã‚‹\n",
    "    for col in feature_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"ç‰¹å¾´é‡ç”Ÿæˆé–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. äºˆæ¸¬ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_race(race_data: Dict) -> Dict:\n",
    "    \"\"\"ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ\"\"\"\n",
    "    df = create_features(race_data)\n",
    "    \n",
    "    if df.empty:\n",
    "        return None\n",
    "    \n",
    "    # ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "    X = df[feature_columns].copy()\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "    X_scaled = pd.DataFrame(\n",
    "        scaler.transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index,\n",
    "    )\n",
    "    \n",
    "    # äºˆæ¸¬\n",
    "    scores = model.predict(X_scaled)\n",
    "    \n",
    "    # ç¢ºç‡è¨ˆç®—ï¼ˆã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ï¼‰\n",
    "    exp_scores = np.exp(scores - np.max(scores))\n",
    "    probabilities = exp_scores / exp_scores.sum()\n",
    "    \n",
    "    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨\n",
    "    if calibrator is not None:\n",
    "        pred_min = scores.min()\n",
    "        pred_max = scores.max()\n",
    "        if pred_max > pred_min:\n",
    "            scores_normalized = (scores - pred_min) / (pred_max - pred_min)\n",
    "            calibrated_probs = calibrator.predict(scores_normalized)\n",
    "            total = calibrated_probs.sum()\n",
    "            if total > 0:\n",
    "                probabilities = calibrated_probs / total\n",
    "    \n",
    "    # çµæœã‚’æ•´å½¢\n",
    "    results = []\n",
    "    entries = race_data.get(\"entries\", [])\n",
    "    \n",
    "    for i, entry in enumerate(entries):\n",
    "        results.append({\n",
    "            \"horse_number\": entry.get(\"horse_number\"),\n",
    "            \"horse_name\": entry.get(\"horse_name\", \"ä¸æ˜\"),\n",
    "            \"score\": float(scores[i]),\n",
    "            \"probability\": float(probabilities[i]),\n",
    "            \"odds\": entry.get(\"odds\"),\n",
    "            \"expected_value\": float(probabilities[i] * (entry.get(\"odds\", 1) or 1)),\n",
    "        })\n",
    "    \n",
    "    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    # é †ä½ä»˜ã‘\n",
    "    for i, r in enumerate(results):\n",
    "        r[\"pred_rank\"] = i + 1\n",
    "    \n",
    "    return {\n",
    "        \"race_id\": race_data[\"race_id\"],\n",
    "        \"race_name\": race_data.get(\"race_name\", \"\"),\n",
    "        \"course\": race_data.get(\"course\", \"\"),\n",
    "        \"predictions\": results,\n",
    "    }\n",
    "\n",
    "print(\"äºˆæ¸¬é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# å…¨ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ\n",
    "all_predictions = []\n",
    "\n",
    "for race_info in tqdm(races, desc=\"äºˆæ¸¬å®Ÿè¡Œä¸­\"):\n",
    "    try:\n",
    "        # å‡ºé¦¬è¡¨ã‚’å–å¾—\n",
    "        race_data = scrape_shutuba(race_info[\"race_id\"])\n",
    "        \n",
    "        # äºˆæ¸¬å®Ÿè¡Œ\n",
    "        prediction = predict_race(race_data)\n",
    "        \n",
    "        if prediction:\n",
    "            all_predictions.append(prediction)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: {race_info['race_id']} - {e}\")\n",
    "\n",
    "print(f\"\\n{len(all_predictions)}ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. äºˆæ¸¬çµæœã®è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction(pred: Dict):\n",
    "    \"\"\"äºˆæ¸¬çµæœã‚’è¡¨ç¤º\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{pred['course']} {pred['race_name']}\")\n",
    "    print(f\"ãƒ¬ãƒ¼ã‚¹ID: {pred['race_id']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'é †ä½':>4} {'é¦¬ç•ª':>4} {'é¦¬å':<12} {'å‹ç‡':>8} {'ã‚ªãƒƒã‚º':>8} {'æœŸå¾…å€¤':>8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for p in pred[\"predictions\"][:5]:  # ä¸Šä½5é ­\n",
    "        prob_pct = p[\"probability\"] * 100\n",
    "        odds_str = f\"{p['odds']:.1f}\" if p['odds'] else \"-\"\n",
    "        ev = p[\"expected_value\"]\n",
    "        ev_mark = \"â˜…\" if ev >= 1.0 else \"\"\n",
    "        \n",
    "        print(f\"{p['pred_rank']:>4} {p['horse_number']:>4} {p['horse_name']:<12} {prob_pct:>7.1f}% {odds_str:>8} {ev:>7.2f}{ev_mark}\")\n",
    "\n",
    "# å„ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ã‚’è¡¨ç¤º\n",
    "for pred in all_predictions:\n",
    "    display_prediction(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. äºˆæ¸¬çµæœã®ä¿å­˜ï¼ˆSupabaseï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_predictions_to_supabase(predictions: List[Dict]) -> int:\n",
    "    \"\"\"äºˆæ¸¬çµæœã‚’Supabaseã«ä¿å­˜\"\"\"\n",
    "    saved = 0\n",
    "    \n",
    "    for pred in predictions:\n",
    "        try:\n",
    "            data = {\n",
    "                \"race_id\": pred[\"race_id\"],\n",
    "                \"model_version\": MODEL_VERSION,\n",
    "                \"results_json\": pred[\"predictions\"],\n",
    "            }\n",
    "            \n",
    "            # æ—¢å­˜ãƒã‚§ãƒƒã‚¯\n",
    "            existing = supabase.table(\"predictions\").select(\"id\").eq(\"race_id\", pred[\"race_id\"]).eq(\"model_version\", MODEL_VERSION).execute()\n",
    "            \n",
    "            if existing.data:\n",
    "                # æ›´æ–°\n",
    "                supabase.table(\"predictions\").update(data).eq(\"id\", existing.data[0][\"id\"]).execute()\n",
    "            else:\n",
    "                # æ–°è¦\n",
    "                supabase.table(\"predictions\").insert(data).execute()\n",
    "            \n",
    "            saved += 1\n",
    "        except Exception as e:\n",
    "            print(f\"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {pred['race_id']} - {e}\")\n",
    "    \n",
    "    return saved\n",
    "\n",
    "# äºˆæ¸¬ã‚’ä¿å­˜\n",
    "saved_count = save_predictions_to_supabase(all_predictions)\n",
    "print(f\"\\n{saved_count}ä»¶ã®äºˆæ¸¬ã‚’Supabaseã«ä¿å­˜ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ä½¿ã„æ–¹ã¾ã¨ã‚\n",
    "\n",
    "### ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆã®é‹ç”¨ãƒ•ãƒ­ãƒ¼\n",
    "\n",
    "1. **ãƒ­ãƒ¼ã‚«ãƒ«FastAPIï¼ˆé‡ã„å‡¦ç†ï¼‰**\n",
    "   - ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’å®Ÿè¡Œ\n",
    "   - å­¦ç¿’å®Œäº†å¾Œã€Supabase Storageã«è‡ªå‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "   - `POST /api/v1/model/retrain-and-upload`\n",
    "\n",
    "2. **ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆè»½ã„å‡¦ç†ï¼‰**\n",
    "   - Supabase Storageã‹ã‚‰æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "   - å½“æ—¥ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "   - äºˆæ¸¬ã‚’å®Ÿè¡Œ\n",
    "   - çµæœã‚’Supabaseã«ä¿å­˜\n",
    "\n",
    "### ãƒ¡ãƒªãƒƒãƒˆ\n",
    "- Colabã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’æ°—ã«ã›ãšã«é‡ã„å­¦ç¿’ãŒå¯èƒ½\n",
    "- Colabã¯è»½é‡ãªæ¨è«–ã®ã¿ â†’ ç„¡æ–™æ ã§ååˆ†\n",
    "- ã©ã“ã‹ã‚‰ã§ã‚‚æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬å¯èƒ½"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
